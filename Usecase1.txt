from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, count, avg, split, desc
from datetime import datetime, timedelta


spark = SparkSession.builder.appName("LogFileAnalysis").getOrCreate()

df = spark.read.format("json").option("multiline", True).load("dbfs:/FileStore/shared_uploads/yaswanth.velamati@gmail.com/sample_logs.json")

df_1 = df.filter(col("timestamp").between(start_date, today))

df_1.show()

 Task 1
error_logs = df_1.filter(col("log_level") == "ERROR")
top_3_servers = error_logs.groupBy("server_id").count().orderBy(desc("count")).limit(3)
print("Top 3 servers with the highest number of ERROR logs:")
top_3_servers.show()

Task 2
logs_df = df_1.withColumn("date", to_date(col("timestamp"))) 
daily_logs = logs_df.groupBy("server_id", "date").count()

daily_logs.show()

avg_logs_per_day = daily_logs.groupBy("server_id").agg(avg("count").alias("avg_logs_per_day"))
print("\nAverage number of logs generated per day by each server:")
avg_logs_per_day.show()

Task 3

log_summary = logs_df.groupBy("log_level", "message").count().orderBy("log_level", desc("count"))
print("\nMost common log messages for each severity level:")
log_summary.show()